{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Platform","text":"<p>Note</p> <p>This documentation is a work in progress and is subject to change. Please check back regularly for updates.</p>"},{"location":"#platform-overview","title":"Platform Overview","text":"<p>Built for DevOps, Platform Engineers, and SREs, this solution streamlines cluster management by eliminating operational overhead, automating deployments at scale, and enforcing consistency across environments\u2014whether using a distributed or hub-and-spoke architecture.</p> <p>Reduce complexity, embrace automation, and accelerate delivery with a scalable, self-healing, and declarative approach to Kubernetes management.</p>"},{"location":"#why-this-pattern","title":"Why This Pattern?","text":"<p>Managing multiple Kubernetes clusters across different environments presents challenges in consistency, scalability, and automation. This solution provides:</p> <ul> <li>Standardized provisioning \u2013 Automate cluster creation with Infrastructure as Code (IaC).</li> <li>GitOps-based management \u2013 Declarative, version-controlled deployments using ArgoCD.</li> <li>Flexible architectures \u2013 Support for both distributed and hub-and-spoke models.</li> <li>Secure multi-cluster operations \u2013 Enforce policies, RBAC, and secrets management at scale.</li> <li>Tenants Applications - Provides tenants consumers an easy way to onboard their workloads.</li> </ul>"},{"location":"#platform-tenets","title":"Platform Tenets","text":"<p>Too often, platforms are designed from a purely technical standpoint, packed with cutting-edge tools and complex abstractions\u2014yet they fail to deliver a great developer experience. They become rigid, overwhelming, and unintuitive, forcing teams to navigate layers of complexity just to deploy and operate their workloads.</p> <p>This is where strong platform tenets come in.</p> <ul> <li>Treat the platform as a product, not just infrastructure\u2014it should have clear users, a roadmap, and continuous improvements.</li> <li>Focus on developer experience make workflows intuitive and efficient.</li> <li>Provide self-service capabilities for developers to deploy and manage workloads independently.</li> <li>Ensure guardrails, not gates\u2014provide secure defaults but allow flexibility when needed.</li> <li>Optimize for usability and maintainability, not just technical capability.</li> <li>Reduce cognitive load by abstracting unnecessary infrastructure details.</li> <li>Follow opinionated defaults but allow extensibility for advanced use cases.</li> </ul>"},{"location":"architecture/overview/","title":"Architecture","text":"<p>The platform currently support both a standalone and hub and spoke architecture.</p>"},{"location":"architecture/overview/#repository-structure","title":"Repository Structure","text":"<p>The platform is composed of two main repositories that work together, This two-repository approach enables:</p> <ul> <li>Clear separation of platform and environment-specific concerns</li> <li>Reuse of core platform components across different deployments</li> <li>Independent versioning of platform and tenant configurations</li> <li>Simplified environment management and promotion</li> <li>Standardized yet flexible application deployment patterns</li> </ul> <p>The tenancy repository imports the platform repository as a dependency, allowing it to leverage the core capabilities while adding environment-specific customizations and applications.</p>"},{"location":"architecture/overview/#deployment-options","title":"Deployment Options","text":"<p>Currently we support two deployment options, standalone or hub and spoke.</p>"},{"location":"architecture/overview/#standalone-installation","title":"Standalone Installation","text":"<p>In a standalone installation, the platform is deployed to a single EKS cluster and manages itself. This self-hosted approach means the platform components (ArgoCD, controllers, etc.) run within the same cluster they are managing.</p> <p>Key characteristics of standalone mode:</p> <ul> <li>Single cluster deployment - Platform runs in the same cluster it manages</li> <li>Self-hosted GitOps - ArgoCD manages its own configuration and other platform components</li> <li>Simplified architecture - No external dependencies or control plane</li> <li>Suitable for:</li> <li>Development/testing environments</li> <li>Small-scale production deployments</li> <li>Single-cluster use cases</li> </ul> <p>The standalone mode provides a simpler starting point while still delivering the core platform capabilities. It can later be expanded into a hub-spoke model as needs grow.</p>"},{"location":"architecture/overview/#hub-spoke","title":"Hub &amp; Spoke","text":"<p>The hub and spoke architecture provides a centralized control plane for managing multiple Kubernetes clusters. In this model, a dedicated management cluster (the \"hub\") hosts the platform components and orchestrates deployments across multiple workload clusters (the \"spokes\").</p> <p>Key characteristics of hub-spoke mode:</p> <ul> <li>Centralized management - Platform components run in a dedicated control plane cluster</li> <li>Multi-cluster orchestration - Single hub can manage many spoke clusters</li> <li>Separation of concerns - Clear distinction between management and workload clusters</li> <li>Enhanced scalability - Easier to add/remove clusters without affecting platform</li> <li>Suitable for:</li> <li>Enterprise environments</li> <li>Multi-cluster/multi-region deployments</li> <li>Production workloads requiring strict separation</li> </ul> <p>The hub cluster runs ArgoCD and other platform components, which then manage the configuration and deployments across all registered spoke clusters. This provides a single point of control while maintaining isolation between the management plane and workload environments.</p>"},{"location":"architecture/setup/","title":"Platform Setup","text":""},{"location":"architecture/setup/#overview","title":"Overview","text":"<p>The platform bootstrapping process follows a structured approach to set up a complete Kubernetes environment with all necessary components. This document outlines the key steps and components involved in bootstrapping the platform.</p>"},{"location":"architecture/setup/#cluster-definition","title":"Cluster Definition","text":"<p>The cluster definition contains the configuration for the platform and tenant repositories, along with metadata about the cluster. This definition drives what features are enabled and how the platform is configured.</p> <pre><code>## The name of the tenant cluster\ncluster_name: dev\n## The cloud vendor to use for the tenant cluster\ncloud_vendor: aws\n## The environment to use for the tenant cluster\nenvironment: development\n## The repository containing the tenant configuration\ntenant_repository: https://github.com/gambol99/platform-tenant.git\n## The revision to use for the tenant repository\ntenant_revision: HEAD\n## The path inside the tenant repository to use for the tenant cluster\ntenant_path: \"\"\n## The repository containing the platform configuration\nplatform_repository: https://github.com/gambol99/kubernetes-platform.git\n## The revision to use for the platform repository\nplatform_revision: HEAD\n## The path inside the platform repository\nplatform_path: \"\"\n## The type of cluster we are (standalone, spoke or hub)\ncluster_type: standalone\n## The name of the tenant\ntenant: tenant_name\n## We use labels to enable/disable features in the tenant cluster\nlabels:\n  enable_cert_manager: \"true\"\n  enable_external_dns: \"false\"\n  enable_external_secrets: \"true\"\n  enable_kro: \"true\"\n  enable_kyverno: \"true\"\n  enable_metrics_server: \"true\"\n</code></pre>"},{"location":"architecture/setup/#provisioning-a-cluster","title":"Provisioning a Cluster","text":"<p>Once we have provisioned a Kubernetes cluster and installed ArgoCD, the values from this cluster definition are used to create the initial bootstrap Application. This application is fed the following</p> <ul> <li>Platform Repository and Revision: the location of the platform (i.e. this repository) and the version of the platform you want to run.</li> <li>Tenant Repository: the location of the tenant repository, which contains the cluster definitions, and tenants applications.</li> </ul> <p>See the Standalone Example Repository</p>"},{"location":"architecture/setup/#bootstrap-application","title":"Bootstrap Application","text":"<p>In the example terraform we extract and the required information from the cluster definition being build and use that to template the bootstrap application.</p> <pre><code>locals {\n  ## The cluster configuration\n  cluster = yamldecode(file(var.cluster_path))\n  ## The cluster_name of the cluster\n  cluster_name = local.cluster.cluster_name\n  ## The cluster type\n  cluster_type = local.cluster.cluster_type\n  ## The platform repository\n  platform_repository = local.cluster.platform_repository\n  ## The platform revision\n  platform_revision = local.cluster.platform_revision\n  ## The tenant repository\n  tenant_repository = local.cluster.tenant_repository\n}\n\n## Provision and bootstrap the platform using an tenant repository\nmodule \"platform\" {\n  count = local.enable_platform ? 1 : 0\n  source = \"github.com/gambol99/terraform-aws-eks//modules/platform?ref=main\"\n\n  ## Name of the cluster\n  cluster_name = local.cluster_name\n  # The type of cluster\n  cluster_type = local.cluster_type\n  # Any rrepositories to be provisioned\n  repositories = var.argocd_repositories\n  ## The platform repository\n  platform_repository = local.platform_repository\n  # The location of the platform repository\n  platform_revision = local.platform_revision\n  # The location of the tenant repository\n  tenant_repository = local.tenant_repository\n  # You pretty much always want to use the HEAD\n  tenant_revision = local.tenant_revision\n  ## The tenant repository path\n  tenant_path = local.tenant_path\n\n  depends_on = [\n    module.eks\n  ]\n}\n</code></pre> <p>The creates the primary Application witn the cluster, which is used to source in the Platform and the tenant repository.</p> <pre><code>$ kubectl -n argocd get applications bootstrap\nbootstrap                     Synced        Healthy\n</code></pre> <p>Peeking into the Application we see</p> <pre><code>$ kubectl -n argocd get application bootstrap -o yaml | yq .spec\ndestination:\n  namespace: argocd\n  server: https://kubernetes.default.svc\nproject: default\nsource:\n  kustomize:\n    patches:\n      - patch: |\n          - op: replace\n            path: /spec/generators/0/git/repoURL\n            value: https://github.com/gambol99/platform-tenant.git\n          - op: replace\n            path: /spec/generators/0/git/revision\n            value: main\n          - op: replace\n            path: /spec/generators/0/git/files/0/path\n            value: clusters/dev.yaml\n        target:\n          kind: ApplicationSet\n          name: system-platform\n  path: kustomize/overlays/standalone\n  repoURL: https://github.com/gambol99/kubernetes-platform.git\n  targetRevision: main\n</code></pre> <p>The application passes the tenany repository down into the platform as Kustomize patches, alone with the location of the cluster definition. The next important Application created off the back of this is the <code>system-registration</code> Application, which can be found here.</p>"},{"location":"architecture/setup/#cluster-registration","title":"Cluster Registration","text":"<p>The cluster registration Application which is bootstrapped via the above, effectively create an application around the cluster definition, it watches for change to the file and provisions a cluster secret from the values. Hence once the cluster is provisions, all changes go via the cluster definition i.e if you want to add a feature, or change the platform revision.</p> <pre><code>$ kubectl -n argocd get applications system-registration\nNAME                  SYNC STATUS   HEALTH STATUS\nsystem-registration   Synced        Healthy\n</code></pre> <p>This used the charts/cluster-registration, creating the ArgoCD cluster secret from the values.</p> <p>The bootstap also sources in the rest of the Platform application sets which are repository for providing tenant functionality as well sourcing in the platform applications themselves.</p>"},{"location":"architecture/system-applications/","title":"Platform Applications","text":"<p>All of the platform applications are effectively sourced in via these two application sets</p> <ul> <li>Helm Application: is responsible for reading the helm application definition, and installing the applications on all clusters who have the <code>enable_FEATURE</code> label.</li> <li>Kustomize Applications: is responsible for installing any Kustomize applications from the Platform, again using the feature labels as a toggle.</li> </ul>"},{"location":"architecture/system-applications/#helm-applications","title":"Helm Applications","text":"<p>All the helm applications deployable by the platform can be found in the addons directory. For Helm applications</p> <pre><code>$ tree addons/helm -L2\naddons/helm\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 cloud\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aws.yaml\n\u2514\u2500\u2500 oss.yaml\n</code></pre> <p>The application set will sources all the <code>oss.yaml</code> items, and using the <code>{{ .cloud_vendor }}</code> attribute associated to the cluster, the appropriate cloud vendor file. Each of the files is a collection of Helm entries i.e</p> <pre><code>- feature: metrics_server\n  chart: metrics-server\n  repository: https://kubernetes-sigs.github.io/metrics-server\n  version: \"3.12.2\"\n  namespace: kube-system\n</code></pre> <p>The following fields are supported:</p> <ul> <li><code>feature</code>: The feature is a label which must be defined on the cluster definition for the feature to be enabled.</li> <li><code>chart</code>: Optional chart name to use for the release (assuming the repository is a helm repository).</li> <li><code>repository</code>: The repository is the location of a helm repository to install the chart from.</li> <li><code>path</code>: Optional path inside the repository to install the chart from (assuming the repository is a git repository).</li> <li><code>version</code>: The version of the chart to install or the git reference to use (e.g. <code>main</code>, <code>HEAD</code>, <code>v0.1.0</code>).</li> <li><code>namespace</code>: The namespace to install the chart into.</li> </ul> <p>All the fields are optional except for <code>path</code> and <code>chart</code>, as they are dependent on if the repository is a helm repository or a git repository.</p> <p>The values files for the Helm applications can be found in <code>config/NAME/all.yaml</code></p>"},{"location":"architecture/system-applications/#tenant-overrides","title":"Tenant Overrides","text":"<p>Note, while not advisable the tenant repository can override or add to additional values to the base application</p> <pre><code>sources:\n  - repoURL: \"{{ .repository }}\"\n    targetRevision: \"{{ .version }}\"\n    chart: '{{ default \"\" .chart }}'\n    path: '{{ default \"\" .repository_path }}'\n    helm:\n      releaseName: \"{{ default .chart .release_name }}\"\n      ignoreMissingValueFiles: true\n      valueFiles:\n        - \"$tenant/{{ .tenant_path }}/config/{{ .feature }}/all.yaml\"\n        - \"$values/config/{{ .feature }}/all.yaml\"\n\n  - repoURL: \"{{ .metadata.annotations.platform_repository }}\"\n    targetRevision: \"{{ .metadata.annotations.platform_revision }}\"\n    ref: values\n\n  - repoURL: \"{{ .metadata.annotations.tenant_repository }}\"\n    targetRevision: \"{{ .metadata.annotations.tenant_revision }}\"\n    ref: tenant\n</code></pre> <p>As you can see from the Application Set, the tenant repository is also sourced in, and values from any <code>config/NAME/all.yaml</code> will take precedence over the platform version.</p>"},{"location":"architecture/system-appsets/","title":"System ArgoCD Application Sets","text":"<p>All the application set which compose the platform can be found in</p> <ul> <li>The apps/ directory, contains the bulk to the system and tenant appsets.</li> <li>The kustomize/overlays/standalone entrypoint.</li> <li>The kustomize/overlays/hub entrypoint.</li> </ul>"},{"location":"architecture/system-appsets/#platform-application-set","title":"Platform Application Set","text":"<p>The platform application sets are the entrypoint application sets for the standalone and hub cluster types. These can be found under the kustomize/overlays directory. They are solely responsible for sourcing the following application sets details below, applying kustomize patches where required.</p>"},{"location":"architecture/system-appsets/#cluster-registration-application-set","title":"Cluster Registration Application Set","text":"<p>The system-registration and the hub version are responsible for sourcing the cluster definitions from the tenant repository and producing a cluster secret, using the charts/cluster-registration helm chart.</p>"},{"location":"architecture/system-appsets/#system-helm-application-set","title":"System Helm Application Set","text":"<p>The system-helm application set is responsible for installing the core platform components.</p> <p>This application set merges the addons, and then filters the applications using the labels attached within the cluster.</p> <pre><code>generators:\n  - matrix:\n      generators:\n        - git:\n            repoURL: PLATFORM_REPO\n            revision: PLATFORM_REVISION\n            files:\n              - path: \"addons/helm/cloud/{{ .cloud_vendor }}/*.yaml\"\n              - path: \"addons/helm/*.yaml\"\n          selector:\n            matchExpressions:\n              - key: version\n                operator: Exists\n              - key: repository\n                operator: Exists\n              - key: namespace\n                operator: Exists\n        - clusters:\n            selector:\n              matchExpressions:\n                - key: environment\n                  operator: Exists\n                - key: \"enable_{{ .feature }}\"\n                  operator: In\n                  values: [\"true\"]\n</code></pre> <p>The addons are a collection of helm application definitions i.e</p> <pre><code>- feature: metrics_server\n  chart: metrics-server\n  repository: https://kubernetes-sigs.github.io/metrics-server\n  version: \"3.12.2\"\n  namespace: kube-system\n\n- feature: volcano\n  chart: volcano\n  repository: https://volcano-sh.github.io/helm-charts\n  version: \"1.9.0\"\n  namespace: volcano-system\n</code></pre> <p>Assuming the cluster selected has a label <code>enable_metrics_server=true</code> and <code>enable_volcano=true</code> in the cluster definition, the helm applications will be installed.</p> <p>Currently we use the following helm values locations as the source of values to the chart.</p> <pre><code>sources:\n  - repoURL: \"{{ .repository }}\"\n    targetRevision: \"{{ .version }}\"\n    chart: '{{ default \"\" .chart }}'\n    path: '{{ default \"\" .repository_path }}'\n    helm:\n      releaseName: \"{{ normalize (default .feature .release_name) }}\"\n      ignoreMissingValueFiles: true\n      valueFiles:\n        - \"$tenant/{{ .metadata.annotations.tenant_path }}/config/{{ .chart }}/all.yaml\"\n        - \"$values/config/{{ .feature }}/all.yaml\"\n\n  - repoURL: \"{{ .metadata.annotations.platform_repository }}\"\n    targetRevision: \"{{ .metadata.annotations.platform_revision }}\"\n    ref: values\n\n  - repoURL: \"{{ .metadata.annotations.tenant_repository }}\"\n    targetRevision: \"{{ .metadata.annotations.tenant_revision }}\"\n    ref: tenant\n</code></pre> <p>Tenant Overrides</p> <p>Note from the above configuration it is technically possible to override the values for the helm chart by adding a <code>all.yaml</code> file to the tenant repository, under a similar folder structure i.e. config/metrics-server/all.yaml.</p>"},{"location":"architecture/system-appsets/#helm-values-and-configuration","title":"Helm Values and Configuration","text":"<p>The configuration and helm values for the helm add-on's can be found in the config directory. Simply create a folder named after the chart name i.e <code>config/metrics-server</code> and drop an <code>all.yaml</code> file in the folder.</p> <p>By default the helm values will be sourced in the following order:</p> <pre><code>- \"$tenant/{{ .metadata.annotations.tenant_path }}/config/{{ .feature }}/{{ .metadata.labels.cloud_vendor }}.yaml\"\n- \"$tenant/{{ .metadata.annotations.tenant_path }}/config/{{ .feature }}/all.yaml\"\n- \"$values/config/{{ .feature }}/{{ .metadata.labels.cloud_vendor }}.yaml\"\n- \"$values/config/{{ .feature }}/all.yaml\"\n</code></pre> <p>You can find the application set here</p> <p>Another way to pass values to the Helm applications is via <code>parameters</code> i.e</p> <pre><code>- feature: volcano\n  chart: volcano\n  repository: https://volcano-sh.github.io/helm-charts\n  version: \"1.9.0\"\n  namespace: volcano-system\n  parameters:\n    - name: serviceAccount.annotations.test\n      value: default_value\n\n    # Reference metadata from the cluster definition\n    - name: serviceAccount.annotations.test2\n      value: metadata.labels.cloud_vendor\n</code></pre>"},{"location":"architecture/system-appsets/#helm-values-key-points","title":"Helm Values Key Points","text":"<ul> <li>The order of precedence is tenant overrides, cloud specific (i.e. <code>kind</code>, <code>aws</code>), followed by the default values in <code>all.yaml</code>.</li> <li>Next comes platform default values, again following cloud vendor, then defaults; these are located in the <code>config/&lt;FEATURE&gt;</code> directory.</li> <li>Tenant currently have the option to override the platform default, though we're considering dropping this feature in future releases, or at the very least making an optional in the cluster definition.</li> </ul>"},{"location":"architecture/system-appsets/#system-kustomize-application-set","title":"System Kustomize Application Set","text":"<p>The system-kustomize is responsible for provisioning any kustomize related functionality from the system. The application set use's a git generator to source all the <code>kustomize.yml</code> from the addons/kustomize directory.</p> <p>Kustomize applications are defined in a similar manner to helm applications, with the following fields:</p> <pre><code>---\nkustomize:\n  ## Human friendly description\n  description: \"\"\n  ## The feature flag used to enable the feature\n  feature: &lt;FEATURE&gt;\n  ## The path to the kustomize overlay\n  path: base\n  ## Optional patches to apply to the kustomize overlay\n  patches:\n    - target:\n        kind: &lt;KIND&gt;\n        name: &lt;NAME&gt;\n      path: &lt;PATH&gt;\n      key: &lt;KEY&gt;\n      default: &lt;DEFAULT&gt;\n\n  ## Optional labels applied to all resources\n  commonLabels:\n    app.kubernetes.io/managed-by: argocd\n\n  ## Optional annotations applied to all resources\n  commonAnnotations:\n    argocd.argoproj.io/sync-options: Prune=false\n\n## The namespace options\nnamespace:\n  ## The name of the namespace to deploy the application\n  name: kube-system\n\n## Synchronization options\nsync:\n  ## How to order the deployment of the resources\n  phase: primary\n</code></pre> <p>Note, kustomize application support the use of patching, but taking fields from the cluster definitions labels and annotations, and using then as values in the patches.</p> <pre><code>## Optional patches to apply to the kustomize overlay\npatches:\n  - target:\n      kind: Namespace\n      name: test\n    path: /metadata/annotations/environment\n    key: metadata.annotations.environment\n    default: unknown\n</code></pre> <p>In the above example, the <code>metadata.annotations.environment</code> value from the cluster definition will be used as the value for the patch.</p>"},{"location":"architecture/system-appsets/#external-kustomize-repository","title":"External Kustomize Repository","text":"<p>System applications using Kustomize also support the option to source in an external repository. This can be used by definiting the following</p> <pre><code>kustomize:\n  ## The feature used to toggle the addon\n  feature: kyverno\n  ## The path inside the repositor\n  path: kustomize\n  ## External repository, else by default we use the platform repository and revision\n  repository: https://github.com/gambol99/exteranl-repository.git\n  ## The revision for the above repository\n  revision: HEAD\n</code></pre>"},{"location":"architecture/tenant-appsets/","title":"Tenant ArgoCD Application Sets","text":"<p>All tenant application sets can be found in the apps/tenant directory. Similar to the system application sets, these are responsible for sourcing the tenant application definitions and applying kustomize patches where required. Indeed the application definition for applications are almost identical.</p>"},{"location":"architecture/tenant-appsets/#argocd-projects","title":"ArgoCD Projects","text":"<p>While the bulk of the system applications run under the <code>default</code> ArgoCD project, the tenant applications run under a projects <code>tenat-applications</code> and <code>tenant-system</code> depending on whether they are system or standard applications. This used to place restrictions on the namespaces a tenant application can deploy, as well as resources the applications can provision.</p>"},{"location":"architecture/tenant-appsets/#tenant-helm-application-set","title":"Tenant Helm Application Set","text":"<p>The tenant helm application set is similar to the system helm application set, but is responsible for installing the tenant applications. The tenant applications are sourced from the tenant repository.</p> <p>Applications for tenants can be deployed using a GitOps approach directly from the tenant repository. The workloads folder contains two main directories:</p> <p> <code>workloads/applications</code> - Contains standard application definitions that run under the tenant's ArgoCD project with regular permissions</p> <p> <code>workloads/system</code> - Contains system-level application definitions that run under a privileged ArgoCD project with elevated permissions</p> <p>By simply adding Helm charts configurations into the appropriate directory structure, applications can be:</p> <ul> <li>Easily deployed to the cluster</li> <li>Upgraded through GitOps workflows</li> <li>Promoted between environments in a controlled manner</li> </ul> <p>This separation of applications and system components allows for proper access control while maintaining a simple deployment model.</p>"},{"location":"architecture/tenant-appsets/#helm-applications","title":"Helm Applications","text":"<p>You can deploy using a helm chart, by adding a <code>CLUSTER_NAME.yaml</code>.</p> <ol> <li>Create a folder (by default this becomes the namespace)</li> <li>Add a <code>CLUSTER_NAME.yaml</code> file</li> </ol> <pre><code>helm:\n  ## (Optional) The chart to use for the deployment.\n  chart: ./charts/platform\n  ## (Optional) The path inside a repository to the chart to use for the deployment.\n  path: ./charts/platform\n  ## (Required) The release name to use for the deployment.\n  release_name: platform\n  ## (Required) The version of the chart to use for the deployment.\n  version: 0.1.0\n\n## Sync Options\nsync:\n  # (Optional) The phase to use for the deployment, used to determine the order of the deployment.\n  phase: primary|secondary\n  # (Optional) The duration to use for the deployment.\n  duration: 30s\n  # (Optional) The max duration to use for the deployment.\n  max_duration: 5m\n</code></pre> <p>In order to use helm values, you need to create a <code>values.yaml</code> file.</p> <ol> <li>For the helm values, create a folder called <code>values</code> inside the folder you created in step 1.</li> <li>Add a <code>all.yaml</code> file to the values folder, which will be used to deploy the application.</li> </ol>"},{"location":"architecture/tenant-appsets/#tenant-kustomize-application-set","title":"Tenant Kustomize Application Set","text":"<p>The tenant kustomize application set is responsible for provisioning any kustomize related functionality from the tenant. The application set use's a git generator to source all the <code>kustomize.yml</code> from the [workloads](</p> <p>Kustomize applications are defined in a similar manner to helm applications, with the following fields:</p> <pre><code>---\nkustomize:\n  # (Required) The path to the kustomize base.\n  path: kustomize\n  # (Optional) Override the namespace to use for the deployment.\n  namespace: override-namespace\n  # (Optional) Patches to apply to the deployment.\n  patches:\n    - target:\n        kind: Deployment\n        name: frontend\n      patch:\n        - op: replace\n          path: /spec/template/spec/containers/0/image\n          ## This value is looked from the cluster definition.\n          value: metadata.annotations.image\n          ## This is the default value to use if the value is not found.\n          default: nginx:1.21.3\n        - op: replace\n          path: /spec/template/spec/containers/0/version\n          ## This value is looked from the cluster definition.\n          value: metadata.annotations.version\n          ## This is the default value to use if the value is not found.\n          default: \"1.21.3\"\n\n  # (Optional) Common labels to apply to the resources.\n  commonLabels:\n    app.kubernetes.io/managed-by: argocd\n\n  # (Optional) Common annotations to apply to the resources.\n  commonAnnotations:\n    argocd.argoproj.io/sync-options: Prune=false\n</code></pre>"},{"location":"architecture/tenant-appsets/#tenant-system-application-sets","title":"Tenant System Application Sets:","text":"<p>The platform will also deploy an additional applications for tenant system applications i.e. applications created in the <code>workspace/system</code> folder. These applications are deployed under the <code>tenant-system</code> ArgoCD project, which has elevated permissions. Note, these application sets are identical the above but are deployed under a different project, the only reason they are duplicated is at present ArgoCD does not permit to template the project name.</p> <p> tenant-system-helm - Deploys system applications from the tenant repository.</p> <p> tenant-system-kustomize - Deploys system applications from the tenant repository using kustomize.</p>"},{"location":"architecture/tenant-namespace/","title":"Tenant Application Namespaces","text":""},{"location":"architecture/tenant-namespace/#overview","title":"Overview","text":"<p>When deploying applications in either <code>workloads/applications/</code> or <code>workloads/system/</code>, namespaces provisioned will automatically receive a default bundle of Kubernetes resources. This standardizes namespace configuration and security across the platform. The default resources include network policies, RBAC resources, and ingress rules, ensure consistent security controls and access patterns.</p> <p>You can find the default manifests here</p>"},{"location":"catalog/features/","title":"Product Features","text":""},{"location":"catalog/features/#features","title":"Features","text":"<p>The following provides a collection of addons, description and the feature flag (i.e. a label within the cluster definition) to enable it.</p> Chart Namespace Description Feature Flag metrics-server kube-system Collects resource metrics from Kubelets for pod autoscaling and kubectl top enable_metrics_server volcano volcano-system Batch system and job scheduler for high-performance workloads enable_volcano cert-manager cert-manager Automatic certificate management for Kubernetes enable_cert_manager cluster-autoscaler kube-system Automatically adjusts the size of Kubernetes clusters enable_cluster_autoscaler external-dns kube-system Synchronizes exposed Services and Ingresses with DNS providers enable_external_dns external-secrets external-secrets Integrates external secret management systems with Kubernetes enable_external_secrets argo-cd argocd Declarative GitOps continuous delivery tool enable_argocd argo-events argocd Event-driven workflow automation framework enable_argo_events argo-rollouts argocd-rollouts Progressive delivery controller for Kubernetes enable_argo_rollouts argo-workflows argo-workflows Workflow engine for Kubernetes enable_argo_workflows ingress-nginx ingress-nginx Ingress controller for Kubernetes using NGINX enable_ingress_nginx keda keda-system Kubernetes-based Event Driven Autoscaling enable_keda kro kro-system Kubernetes Resource Operator for custom resources enable_kro kube-prometheus-stack prometheus Monitoring and alerting stack including Prometheus, Grafana, and Alertmanager enable_kube_prometheus_stack kyverno kyverno-system Kubernetes native policy management enable_kyverno prometheus-adapter prometheus Adapter to use Prometheus metrics with Kubernetes HPA enable_prometheus_adapter secrets-store-csi-driver kube-system Interface between Kubernetes and external secret stores enable_secrets_store_csi_driver vpa kube-system Vertical Pod Autoscaler for automatic CPU and memory requests enable_vpa"},{"location":"catalog/overview/","title":"Catalog Overview","text":""},{"location":"development/hub/","title":"Hub &amp; Spoke","text":""},{"location":"development/hub/#overview","title":"Overview","text":"<p>When developing features that involve hub and spoke cluster configurations, you'll need at least two Kubernetes clusters:</p> <ol> <li>A hub cluster that serves as the central management plane</li> <li>One or more spoke clusters that are managed by the hub</li> </ol> <p>This setup allows you to test:</p> <ul> <li>Cross-cluster communication and management</li> <li>Application deployment across clusters</li> <li>Policy enforcement and governance</li> <li>Multi-cluster observability</li> <li>Resource federation and sharing</li> </ul>"},{"location":"development/hub/#architecture","title":"Architecture","text":"<p>In the Hub-Spoke model, a centralized hub cluster is designated to host the Argo CD instance. This hub cluster serves as the control center and is responsible for managing resources across multiple spoke clusters. The spoke clusters, which could be numerous (e.g., 5 or 10), rely on the hub cluster for coordination and deployment. The Argo CD instance in the hub cluster is configured to oversee and make changes to the other clusters, offering a centralized and streamlined approach to cluster management.</p>"},{"location":"development/hub/#prerequisites","title":"Prerequisites","text":"<p>The following tools need to be installed on your local machine:</p> Tool Description Installation Link kubectl The Kubernetes command-line tool Install Guide helm The package manager for Kubernetes Install Guide terraform Infrastructure as Code tool for cloud resources Install Guide"},{"location":"development/hub/#setup-the-environments","title":"Setup the Environments","text":"<p>The <code>release</code> folder is used to mimic a tenant repository locally. Under <code>release/hub-aws</code> you will find the necessary files required to manage, test and develop with the hub and spoke setup.</p> <pre><code>$ tree release/hub-aws/ -L 1\nrelease/hub-aws/\n\u251c\u2500\u2500 clusters\n\u251c\u2500\u2500 config\n\u2514\u2500\u2500 workloads\n</code></pre> <ol> <li>We have two cluster definitions hub.yaml and spoke.yaml</li> <li>Note, similar to local development with local and the standalone development, the revision found in the cluster definitions are overridden, allowing you to use your current branch to validate changes.</li> </ol>"},{"location":"development/hub/#provision-the-hub-cluster","title":"Provision the Hub cluster","text":"<ol> <li>Take a look at the terraform code in the <code>terraform</code> directory, and update any <code>terraform/variables/hub.tfvars</code> you feel need changing.</li> <li>Run the Makefile <code>make hub-aws</code>; this will run <code>terraform init</code>, <code>terraform workspace select hub</code> and provision the cluster.</li> <li>Once the Hub has been provision <code>aws eks update-kubeconfig --name hub</code> and verify everything is working, we should see a collection of applications similar to below</li> </ol> <p>Note</p> <p>You can view what the Makefile target is doing by opening the <code>terraform/Makefile</code></p> <pre><code>kubectl -n argocd get applications\n$ k -n argocd get applications\nNAME                                 SYNC STATUS   HEALTH STATUS\nbootstrap                            Synced        Healthy\nplatform                             Synced        Healthy\nsystem-argocd-hub                    Synced        Healthy\nsystem-aws-ack-iam-spoke             Unknown       Unknown\nsystem-cert-manager-spoke            Unknown       Unknown\nsystem-external-secrets-hub          Synced        Healthy\nsystem-external-secrets-spoke        Unknown       Unknown\nsystem-kro-spoke                     Unknown       Unknown\nsystem-kust-cert-manager-spoke       Unknown       Unknown\nsystem-kust-external-secrets-hub     Synced        Healthy\nsystem-kust-external-secrets-spoke   Unknown       Unknown\nsystem-kust-karpenter-hub            Synced        Healthy\nsystem-kust-karpenter-spoke          Unknown       Unknown\nsystem-kust-kyverno-hub              OutOfSync     Healthy\nsystem-kust-kyverno-spoke            Unknown       Unknown\nsystem-kust-priority-hub             Synced        Healthy\nsystem-kust-priority-spoke           Unknown       Unknown\nsystem-kyverno-hub                   Synced        Healthy\nsystem-kyverno-spoke                 Unknown       Unknown\nsystem-registration-hub              Synced        Healthy\nsystem-registration-spoke            Synced        Healthy\n</code></pre> <p>The <code>Unknown</code> is expected, as the cluster is enabled in the cluster definition, but no server endpoint or authentication has been supplied to ArgoCD yet.</p>"},{"location":"development/hub/#provision-the-spoke","title":"Provision the Spoke","text":"<p>With the Hub cluster provisioned we can now move forward and provision our first spoke cluster (we can consider this one dev).</p> <ol> <li>Update any required changes (VPC cidr etc) in the <code>terraform/variables/spoke.tfvars</code>.</li> <li>Run the Makefile target <code>make spoke-aws</code>; against this will initialized the terraform, select a workspace <code>spoke</code> and run terraform apply.</li> <li>We should have an empty EKS cluster now for the spoke.</li> </ol> <p>Now we have to configure the authentication from the Hub to the Spoke.</p>"},{"location":"development/hub/#cluster-authentication","title":"Cluster Authentication","text":"<p>Authentication between the Hub ArgoCD and the spoke cluster is performed using pod identity. Each spoke has cross account role provisioned when the <code>var.hub_account_id</code> is defined. A trust policy is created allowing Hub account to assume this role, and via access entities this role is assumed to cluster administrator.</p> <p>Note, the is nothing to stop you using another method, ArgoCD supports the following https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#clusters mode. You can use the <code>cluster_authentication.config</code> in the cluster definition and fill in any of the above methods of authentication.</p> <p>Below is the cross account role.</p> <pre><code>## Create IAM Role for ArgoCD cross-account access\nresource \"aws_iam_role\" \"argocd_cross_account_role\" {\n  count = local.enable_cross_account_role ? 1 : 0\n\n  name = \"${var.cluster_name}-argocd-cross-account\"\n  tags = local.tags\n\n  # Trust policy that allows the ArgoCD account to assume this role\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\",\n    Statement = [\n      {\n        Sid    = \"AllowCrossAccountAssumeRole\"\n        Effect = \"Allow\",\n        Principal = {\n          AWS = format(\"arn:aws:iam::%s:root\", var.hub_account_id)\n        },\n        Action    = \"sts:AssumeRole\",\n        Condition = {}\n      }\n    ]\n  })\n}\n</code></pre> <p>Note</p> <p>You could bypass the updating of the cluster configuration by simply added an known endpoint i.e create route53 domain and use the EKS endpoint as a CNAME</p> <p>On the Hub cluster pod identity allows the ArgoCD service account to assume the roles in the spoke accounts. The only manually change currently is adding in the Kubernetes endpoints URL post the creation.</p> <ol> <li>Once a spoke have been provisioned, you will have the Kubernetes endpoints URL in the terraform outputs or AWS Console.</li> <li>We need to update the cluster definition in the spoke to include this information i.e. below</li> </ol> <pre><code>## The authentication details for the cluster\ncluster_authentication:\n  ## The server to use for the cluster\n  server: UPDATE_THE_EKS_ENDPOINT\n  ## The configuration to assume the role using the cross account role\n  config:\n    awsAuthConfig:\n      clusterName: spoke\n      roleARN: arn:aws:iam::UPDATE_SPOKE_ACCOUNT_ID:role/spoke-argocd-cross-account\n    tlsClientConfig:\n      insecure: true\n</code></pre> <p>ArgoCD in the hub should now have everything it needs to communicate with spoke.</p> <ol> <li>Go back to the Hub cluster and run <code>kubectl -n argocd get applications</code> you should see the spoke applications now.</li> </ol> <pre><code>$ kubectl -n argocd get applications\nNAME                                 SYNC STATUS   HEALTH STATUS\nbootstrap                            Synced        Healthy\nplatform                             Synced        Healthy\nsystem-argocd-hub                    Synced        Healthy\nsystem-aws-ack-iam-spoke             Synced        Healthy\nsystem-cert-manager-spoke            Synced        Healthy\nsystem-external-secrets-hub          Synced        Healthy\nsystem-external-secrets-spoke        Synced        Healthy\nsystem-kro-spoke                     Synced        Healthy\nsystem-kust-cert-manager-spoke       Synced        Healthy\nsystem-kust-external-secrets-hub     Synced        Healthy\nsystem-kust-external-secrets-spoke   Synced        Healthy\nsystem-kust-karpenter-hub            Synced        Healthy\nsystem-kust-karpenter-spoke          Synced        Healthy\nsystem-kust-kyverno-hub              OutOfSync     Healthy\nsystem-kust-kyverno-spoke            Synced        Healthy\nsystem-kust-priority-hub             Synced        Healthy\nsystem-kust-priority-spoke           Synced        Healthy\nsystem-kyverno-hub                   Synced        Healthy\nsystem-kyverno-spoke                 Synced        Healthy\nsystem-registration-hub              Synced        Healthy\nsystem-registration-spoke            Synced        Healthy\n</code></pre> <p>To start deploying applications to the spoke cluster, you can use the <code>workloads</code> directory in the <code>release/hub-aws</code> directory. View the workloads documentation for more information.</p>"},{"location":"development/local/","title":"Local Development","text":""},{"location":"development/local/#overview","title":"Overview","text":"<p>For the purposes of local development, provisioning Kubernetes clusters in the cloud can be expensive and time consuming. And while not all the features can be fully tested locally, i.e. those relying on cloud resources, pod identity and applications (load balancers for example) and so forth, much of the development of the platform team can be done locally on their machine.</p>"},{"location":"development/local/#prerequisites","title":"Prerequisites","text":"<p>The following tools need to be installed on your local machine:</p> Tool Description Installation Link kubectl The Kubernetes command-line tool Install Guide helm The package manager for Kubernetes Install Guide kind Tool for running local Kubernetes clusters using Docker containers Install Guide <p>We also require the ArgoCD Helm repository setup via</p> <pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\n</code></pre>"},{"location":"development/local/#provisioning-a-local-cluster-standalone","title":"Provisioning a Local Cluster (Standalone)","text":"<p>Within the platform repository we have a <code>release</code> folder, which can be used to simulate a tenant repository, keeping all the development within the same repository.</p> <pre><code>$ tree release -L2\nrelease\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 hub\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 clusters\n\u2514\u2500\u2500 standalone\n    \u251c\u2500\u2500 clusters\n    \u251c\u2500\u2500 config\n    \u2514\u2500\u2500 workloads\n</code></pre>"},{"location":"development/local/#expected-development-workflow","title":"Expected Development Workflow","text":"<p>As a platform engineer you need to validate a new feature on a standalone deployment model.</p> <ol> <li>Create a branch for the feature <code>git checkout -b feat/my-feature</code></li> <li>Note, you don't need to change the branch in the <code>release/standalone/clusters/dev.yaml</code>, as when using local development this revision is overridden, and uses your current git branch.</li> <li>Commit the changes and run <code>make standalone</code></li> <li>The makefile will build a Kubernetes cluster in kind, using the <code>release/standalone</code> and the base tenant repository.</li> <li>Any changes made to the branch will be reflected within the cluster, and changes are polled from the branch.</li> <li>Once the changes have been tested, we can merge to main.</li> </ol> <p>Note, if want validate changes between multiple clusters you can provision another cluster via <code>scripts/make-dev -c prod</code>.</p> <p>Additional clusters can be added, simply by adding a new cluster to the <code>release/standalone/clusters</code> directory, and calling</p> <pre><code>scripts/make-dev.sh --cluster-name &lt;NAME&gt;\n</code></pre>"},{"location":"development/local/#cleanup-the-cluster","title":"Cleanup the Cluster","text":"<p>To cleanup the cluster, run <code>make clean</code></p>"},{"location":"development/overview/","title":"Remote Development","text":""},{"location":"development/overview/#overview","title":"Overview","text":"<p>While iterating using Kind can cover a large portion of the development cycle, there are certain aspects that need to be tested in a live cluster environment. These include:</p> <ul> <li>Pod identity and IAM role integration</li> <li>Cloud provider-specific add-ons and services</li> <li>Load balancer and ingress controller behavior</li> <li>Storage class and volume provisioning</li> <li>Auto-scaling and cluster autoscaler functionality</li> <li>Cloud-specific networking features</li> <li>Service mesh integrations</li> <li>Monitoring and logging infrastructure</li> </ul> <p>For these scenarios, it's recommended to:</p> <ol> <li>Use a dedicated development/test cluster in your cloud provider</li> <li>Keep the cluster isolated from production workloads</li> <li>Enable necessary cloud provider features and add-ons</li> <li>Configure appropriate IAM roles and permissions</li> <li>Test with realistic workload patterns</li> <li>Monitor costs and clean up test resources</li> </ol> <p>This allows you to validate platform changes in an environment that closely matches production, while maintaining a safe space for experimentation.</p>"},{"location":"development/standalone/","title":"Provision a Standalone Cluster","text":""},{"location":"development/standalone/#overview","title":"Overview","text":"<p>The following walks through the steps required to provision a standalone cluster/s within AWS. Here we will be using the internal terraform code found in <code>terraform</code> directory to provision the infrastructure.</p>"},{"location":"development/standalone/#validating-change","title":"Validating Change","text":"<p>Requirement: As a platform engineer, you need to provision a development cluster in AWS to validate and test platform changes before merging to the release branch, and potentially incorporated with a release. This involves:</p> <ul> <li>Creating a feature branch for development</li> <li>Provisioning a standalone AWS cluster for testing</li> <li>Validating the changes in an isolated environment  </li> <li>Submitting pull requests once testing is complete</li> </ul>"},{"location":"development/standalone/#provision-the-cluster","title":"Provision the cluster","text":"<p>We can use the <code>terraform</code> code to provision a EKS cluster for us, and onboard the platform.</p> <ol> <li>Create a branch to commit your changes i.e <code>git checkout -b feat/my_change</code>.</li> <li>Open the <code>terraform/variables/dev.tfvars</code>, this can be used to provision a cluster from or copied as a template.</li> <li>Provision a cluster using terraform; note if you are reusing the <code>dev.tfvars</code> you can simply run <code>make standalone-aws</code>.<ol> <li>If using another <code>variables</code> file, you must run terraform manually via <code>terraform apply -var-file=variables/FILE.tfvars</code></li> </ol> </li> <li>Similar to the local development proccess using kind the branch the cluster is provisioned with is overrided, and uses the current branch, as oppposed one defined within the cluster definition.</li> <li>At the point we should have a EKS cluster, which we can access using <code>kubectl</code>.</li> </ol> <pre><code>$ aws eks update-kubeconfig --name dev\nUpdated context arn:aws:eks:eu-west-2:xxx:cluster/dev in /Users/jest/.kube/config\n</code></pre>"},{"location":"development/standalone/#validate-the-cluster","title":"Validate the Cluster","text":"<ol> <li>Check the applications have been provisioned correctly via <code>kubectl -n argocd get applications</code></li> </ol>"},{"location":"development/standalone/#applying-changes","title":"Applying Changes","text":"<p>From here on in, any changes to the branch will automatically be replicated as the source of truth to the cluster, we can validate the changes are working and once happy, raise the pull request.</p>"},{"location":"getting-started/central/","title":"Hub &amp; Spoke Deployment","text":"<p>Coming soon</p>"},{"location":"getting-started/standalone-aws/","title":"Building Standalone Cluster in AWS","text":"<p>Note</p> <p>This documentation is a work in progress and is subject to change. Please check back regularly for updates.</p> <p>This repository includes a terraform pipeline / codebase to validate locally a standalone build within AWS infrastructure. Enabling the platform team to validate any changes, add-ons, configurations and so forth before taking it to a revision.</p>"},{"location":"getting-started/standalone-aws/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS CLI (https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)</li> <li>Kubectl (https://kubernetes.io/docs/tasks/tools/#kubectl)</li> <li>Terraform (https://developer.hashicorp.com/terraform/downloads)</li> </ul>"},{"location":"getting-started/standalone-aws/#terraform","title":"Terraform","text":"<p>The terraform codebase can be found in the <code>terraform</code> directory. This currently uses two modules to handle the provisioning.</p> <ul> <li>terraform-aws-eks - This module provisions the EKS cluster, networking, iam roles and so on.</li> <li>terraform-kube-platform - This module provisions the platform components, such as ArgoCD and bootstraps the cluster with the platform.</li> </ul>"},{"location":"getting-started/standalone/","title":"Standalone Deployment","text":"<p>Note</p> <p>This documentation is a work in progress and is subject to change. Please check back regularly for updates.</p> <p>The following tries to depict a typical standalone deployment. You can find the example code base for this walk-through at https://github.com/gambol99/platform-tenant</p>"},{"location":"getting-started/standalone/#example-scenerio","title":"Example Scenerio","text":"<p>Using the following scenario, we have</p> Feature Description Multiple Environments Two Kubernetes clusters (<code>dev</code> and <code>prod</code>) representing different environments in the application life cycle Independent Platform Upgrades Each cluster runs its own version of the platform, allowing independent platform upgrades and testing GitOps Workflows Application teams can deploy and manage their applications using GitOps workflows Controlled Promotion Changes to both platform and applications can be promoted between environments in a controlled manner Version Tracking Platform versions are defined in code, enabling clear tracking of what's running where Team Autonomy Application teams have autonomy to deploy, test, and promote"},{"location":"getting-started/standalone/#required-binaries","title":"Required Binaries","text":"<ul> <li>Kind (https://kind.sigs.k8s.io/docs/user/quick-start/#installation)</li> <li>kubectl (https://kubernetes.io/docs/tasks/tools/#kubectl)</li> <li>Terraform (https://developer.hashicorp.com/terraform/downloads)</li> </ul>"},{"location":"getting-started/standalone/#setup-the-environments","title":"Setup the Environments","text":"<p>We need to provision a repository to house the tenant application stack, we can use the template EKS Tenant. Clone repository via</p> <pre><code>gh repo clone gambol99/provision-tenant\n</code></pre> <p>The folder structure of a tenant repository is as follows.</p> <ul> <li>clusters - contains the cluster definitions for both development and production clusters.</li> <li>workloads - contains the ArgoCD application definitions for the tenant applications.</li> <li>workloads/applications - contains the application definitions for standard applications, which run under the tenant project.</li> <li>workloads/platform - contains the application definitions for system applications, which run under a higher privilege.</li> <li>terraform - contains the Terraform code to provision the tenant cluster/s.</li> <li>terraform/variables - contains the Terraform variables for the tenant cluster/s.</li> </ul>"},{"location":"getting-started/standalone/#cluster-definitions","title":"Cluster Definitions","text":"<p>A typical cluster definition is as follows</p> <pre><code>## The name of the tenant cluster\ncluster_name: dev\n## The cloud vendor to use for the tenant cluster\ncloud_vendor: aws\n## The environment to use for the tenant cluster\nenvironment: development\n## The repository containing the tenant configuration\ntenant_repository: https://github.com/gambol99/platform-tenant.git\n## The revision to use for the tenant repository\ntenant_revision: HEAD\n## The path inside the tenant repository to use for the tenant cluster\ntenant_path: \"\"\n## The repository containing the platform configuration\nplatform_repository: https://github.com/gambol99/kubernetes-platform.git\n## The revision to use for the platform repository\nplatform_revision: HEAD\n## The path inside the platform repository\nplatform_path: \"\"\n## The type of cluster we are (standalone, spoke or hub)\ncluster_type: standalone\n## The name of the tenant\ntenant: tenant_name\n## We use labels to enable/disable features in the tenant cluster\nlabels:\n  enable_cert_manager: \"true\"\n  enable_external_dns: \"false\"\n  enable_external_secrets: \"true\"\n  enable_kro: \"true\"\n  enable_kyverno: \"true\"\n  enable_metrics_server: \"true\"\n</code></pre> <p>Noteworthy section are the <code>labels</code>. Label are used by the platform to control whether a piece of functionality is installed via the platform.</p> <p>The following depicts a typical setup using the standalone deployment option. We have an application stacks and te</p> <p>Under this arrangement</p> <ul> <li>ArgoCD is installed on the cluster itself, with all deployments installing on the same cluster.</li> <li>The tenany repository points at the version of the platform, and contains the applications stack.</li> </ul> Feature Description Multiple Environments Two Kubernetes clusters (<code>dev</code> and <code>prod</code>) representing different environments in the application life cycle Independent Platform Upgrades Each cluster runs its own version of the platform, allowing independent platform upgrades and testing GitOps Workflows Application teams can deploy and manage their applications using GitOps workflows Controlled Promotion Changes to both platform and applications can be promoted between environments in a controlled manner Version Tracking Platform versions are defined in code, enabling clear tracking of what's running where Team Autonomy Application teams have autonomy to deploy, test, and promote"},{"location":"platform/overview/","title":"Platform Overview","text":"<p>Note</p> <p>This documentation is a work in progress and is subject to change. Please check back regularly for updates.</p> <p>This section provides an overview of the core features and capabilities of the Kubernetes platform. The platform includes:</p> <ul> <li>Security Controls: Built-in security policies and controls via Kyverno to enforce best practices</li> <li>Multi-Tenancy: Namespace isolation and resource management for multiple teams/workloads</li> <li>GitOps Workflows: Declarative application delivery and configuration management</li> <li>Observability: Integrated monitoring, logging and tracing capabilities</li> <li>Developer Experience: Streamlined workflows and tools for application teams</li> </ul> <p>The following sections detail the specific components and features available in the platform.</p>"},{"location":"platform/security/cilium-examples/","title":"Network Policies with Cilium","text":"<p>Here are some examples of network policies you can create with Cilium:</p>"},{"location":"platform/security/cilium-examples/#example-1-allow-all-traffic-to-a-specific-service","title":"Example 1: Allow All Traffic to a Specific Service","text":"<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"allow-all-to-service\"\nspec:\n  endpointSelector:\n    matchLabels:\n      app: my-service\n  ingress:\n    - fromEntities:\n        - all\n</code></pre>"},{"location":"platform/security/cilium-examples/#example-2-deny-all-traffic-except-from-a-specific-namespace","title":"Example 2: Deny All Traffic Except from a Specific Namespace","text":"<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"deny-all-except-namespace\"\nspec:\n  endpointSelector:\n    matchLabels:\n      app: my-service\n  ingress:\n    - fromEndpoints:\n        - matchLabels:\n            namespace: trusted-namespace\n</code></pre>"},{"location":"platform/security/cilium-examples/#example-3-allow-http-traffic-to-a-specific-path","title":"Example 3: Allow HTTP Traffic to a Specific Path","text":"<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"allow-http-to-path\"\nspec:\n  endpointSelector:\n    matchLabels:\n      app: my-service\n  ingress:\n    - toPorts:\n        - ports:\n            - port: \"80\"\n              protocol: TCP\n          rules:\n            http:\n              - method: \"GET\"\n                path: \"/allowed-path\"\n</code></pre>"},{"location":"platform/security/cilium-examples/#example-4-deny-all-traffic-except-specific-http-methods","title":"Example 4: Deny All Traffic Except Specific HTTP Methods","text":"<pre><code>apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: \"deny-all-except-methods\"\nspec:\n  endpointSelector:\n    matchLabels:\n      app: my-service\n  ingress:\n    - toPorts:\n        - ports:\n            - port: \"80\"\n              protocol: TCP\n          rules:\n            http:\n              - method: \"GET\"\n              - method: \"POST\"\n</code></pre>"},{"location":"platform/security/cilium/","title":"Network Security","text":""},{"location":"platform/security/cilium/#introduction-to-cilium","title":"Introduction to Cilium","text":"<p>Cilium is an open-source software for providing and transparently securing network connectivity and load balancing between application workloads such as application containers or processes.</p> <p>It leverages eBPF (extended Berkeley Packet Filter) technology to provide high-performance networking, security, and observability.</p> <p>Note</p> <p>On EKS Cilium has been configured to use CNI chaining mode (https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/)</p>"},{"location":"platform/security/cilium/#feature-set","title":"Feature Set","text":"<ul> <li>Network Security: Provides fine-grained security policies for network communication between workloads.</li> <li>Load Balancing: Offers efficient load balancing for both east-west and north-south traffic.</li> <li>Network Visibility: Delivers deep visibility into network traffic and application behavior.</li> <li>Transparent Encryption: Ensures data-in-transit encryption without requiring changes to applications.</li> <li>Identity-Aware Security: Uses identity-based security policies rather than relying on IP addresses.</li> <li>Integration with Kubernetes: Seamlessly integrates with Kubernetes for network policy enforcement and service discovery.</li> </ul>"},{"location":"platform/security/cilium/#enabling-cilium","title":"Enabling Cilium","text":"<p>Enabling Cilium on your Kubernetes requires adding the following feature label in the cluster definition:</p> <pre><code>labels:\n  enable_cilium: true\n</code></pre>"},{"location":"platform/security/cilium/#end-to-end-encryption-with-wireguard-and-cilium","title":"End-to-End Encryption with WireGuard and Cilium","text":"<p>End-to-end encryption is a critical feature for ensuring the security and privacy of data as it travels across the network. In our platform, this is achieved using WireGuard in conjunction with Cilium. WireGuard is a modern VPN that is simple, fast, and secure, while Cilium provides networking, security, and observability for cloud-native environments.</p> <p>By default, our platform enables end-to-end encryption using WireGuard and Cilium, ensuring that all data transmitted between nodes is encrypted and protected from unauthorized access. This setup provides robust security with minimal configuration, leveraging the strengths of both WireGuard and Cilium to deliver a seamless and secure networking experience.</p>"},{"location":"platform/security/cilium/#mtls-implementation-in-cilium","title":"mTLS Implementation in Cilium","text":"<p>Cilium implements mutual TLS (mTLS) to provide end-to-end encryption and authentication between workloads. mTLS ensures that both the client and server authenticate each other, providing a higher level of security compared to traditional TLS.</p>"},{"location":"platform/security/cilium/#how-mtls-works-in-cilium","title":"How mTLS Works in Cilium","text":"<ol> <li>Certificate Management: Cilium uses certificates to authenticate workloads. These certificates are typically managed by a certificate authority (CA) and distributed to workloads.</li> <li>Policy Enforcement: Cilium's network policies can specify that mTLS must be used for communication between certain workloads. This ensures that only authenticated and authorized workloads can communicate.</li> <li>Encryption: All data transmitted between workloads is encrypted using mTLS, ensuring data confidentiality and integrity.</li> <li>Integration with eBPF: Cilium leverages eBPF to efficiently enforce mTLS policies and handle encryption/decryption operations at the kernel level, providing high performance and low latency.</li> <li>Traffic Filtering: By intercepting TLS traffic, Cilium can apply fine-grained security policies to filter traffic based on various criteria such as identity, service, and more. When using SPIRE, Cilium can leverage the SPIFFE IDs provided by SPIRE to enforce identity-based policies.</li> </ol> <p>By using mTLS, Cilium provides a robust security mechanism that protects against various network threats, including man-in-the-middle attacks and unauthorized access.</p>"},{"location":"platform/security/kyverno/","title":"Kyverno Policies","text":""},{"location":"platform/security/kyverno/#overview","title":"Overview","text":"<p>Kyverno is a policy engine designed for Kubernetes that validates, mutates, and generates configurations using policies as Kubernetes resources. It provides key features like:</p> <ul> <li>Policy validation and enforcement</li> <li>Resource mutation and generation </li> <li>Image verification and security controls</li> <li>Audit logging and reporting</li> <li>Admission control webhooks</li> </ul> <p>The following policies are shipped by default in this platform to enforce security best practices, resource management, and operational standards.</p> <p>For detailed information about Kyverno's capabilities, refer to the official documentation or policy library.</p>"},{"location":"platform/security/kyverno/#rule-deny-empty-ingress-host","title":"Rule: deny-empty-ingress-host","text":"<p>Category: Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>An ingress resource needs to define an actual host name in order to be valid. This policy ensures that there is a hostname for each rule defined.</p> <p>Rules</p> <ul> <li>disallow-empty-ingress-host (Validation)</li> </ul>"},{"location":"platform/security/kyverno/#rule-require-labels","title":"Rule: require-labels","text":"<p>Category: Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>Define and use labels that identify semantic attributes of your application or Deployment. A common set of labels allows tools to work collaboratively, describing objects in a common manner that all tools can understand. The recommended labels describe applications in a way that can be queried. This policy validates that the labels <code>app.kubernetes.io/name</code>, <code>app.kubernetes.io/version</code>, and <code>app.kubernetes.io/part-of</code> are specified with some value.</p> <p>Rules</p> <ul> <li> <p>check-for-labels (Validation)</p> </li> <li> <p>check-deployment-template-labels (Validation)</p> </li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-no-limits","title":"Rule: deny-no-limits","text":"<p>Category: Best Practices, EKS Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>As application workloads share cluster resources, it is important to limit resources requested and consumed by each Pod. It is recommended to require resource requests and limits per Pod, especially for memory and CPU. If a Namespace level request or limit is specified, defaults will automatically be applied to each Pod based on the LimitRange configuration. This policy validates that all containers have something specified for memory and CPU requests and memory limits.</p> <p>Rules</p> <ul> <li>validate-resources (Validation)</li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-external-secrets","title":"Rule: deny-external-secrets","text":"<p>Category: Security | Severity: medium | Scope: Cluster-wide</p> <p>When provisioning ExternalSecrete, the key must be prefixed with the namespace name to ensure proper isolation and prevent unauthorized access.</p> <p>Rules</p> <ul> <li>namespace-prefix (Validation)</li> <li>Applies to: ExternalSecret</li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-nodeport-service","title":"Rule: deny-nodeport-service","text":"<p>Category: Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>A Kubernetes Service of type NodePort uses a host port to receive traffic from any source. A NetworkPolicy cannot be used to control traffic to host ports. Although NodePort Services can be useful, their use must be limited to Services with additional upstream security checks. This policy validates that any new Services do not use the <code>NodePort</code> type.</p> <p>Rules</p> <ul> <li>validate-nodeport (Validation)</li> </ul>"},{"location":"platform/security/kyverno/#rule-mutate-psa-labels","title":"Rule: mutate-psa-labels","text":"<p>Category: Pod Security Admission, EKS Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>Pod Security Admission (PSA) can be controlled via the assignment of labels at the Namespace level which define the Pod Security Standard (PSS) profile in use and the action to take. If not using a cluster-wide configuration via an AdmissionConfiguration file, Namespaces must be explicitly labeled. This policy assigns the labels <code>pod-security.kubernetes.io/enforce=baseline</code> and <code>pod-security.kubernetes.io/warn=restricted</code> to all new Namespaces if those labels are not included.</p> <p>Rules</p> <ul> <li>add-psa-labels (Mutation)</li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-default-namespace","title":"Rule: deny-default-namespace","text":"<p>Category: Multi-Tenancy | Severity: medium | Scope: Cluster-wide</p> <p>Kubernetes Namespaces are an optional feature that provide a way to segment and isolate cluster resources across multiple applications and users. As a best practice, workloads should be isolated with Namespaces. Namespaces should be required and the default (empty) Namespace should not be used. This policy validates that Pods specify a Namespace name other than <code>default</code>. Rule auto-generation is disabled here due to Pod controllers need to specify the <code>namespace</code> field under the top-level <code>metadata</code> object and not at the Pod template level.</p> <p>Rules</p> <ul> <li> <p>validate-namespace (Validation)</p> </li> <li> <p>validate-podcontroller-namespace (Validation)</p> </li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-latest-image","title":"Rule: deny-latest-image","text":"<p>Category: Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>The ':latest' tag is mutable and can lead to unexpected errors if the image changes. A best practice is to use an immutable tag that maps to a specific version of an application Pod. This policy validates that the image specifies a tag and that it is not called <code>latest</code>.</p> <p>Rules</p> <ul> <li> <p>require-image-tag (Validation)</p> </li> <li> <p>validate-image-tag (Validation)</p> </li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-no-pod-probes","title":"Rule: deny-no-pod-probes","text":"<p>Category: Best Practices, EKS Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>Liveness and readiness probes need to be configured to correctly manage a Pod's lifecycle during deployments, restarts, and upgrades. For each Pod, a periodic <code>livenessProbe</code> is performed by the kubelet to determine if the Pod's containers are running or need to be restarted. A <code>readinessProbe</code> is used by Services and Deployments to determine if the Pod is ready to receive network traffic. This policy validates that all containers have one of livenessProbe, readinessProbe, or startupProbe defined.</p> <p>Rules</p> <ul> <li>deny-no-pod-probes (Validation)</li> </ul>"},{"location":"platform/security/kyverno/#rule-deny-cap-net-raw","title":"Rule: deny-cap-net-raw","text":"<p>Category: Best Practices | Severity: medium | Scope: Cluster-wide</p> <p>Capabilities permit privileged actions without giving full root access. The CAP_NET_RAW capability, enabled by default, allows processes in a container to forge packets and bind to any interface potentially leading to MitM attacks. This policy ensures that all containers explicitly drop the CAP_NET_RAW ability. Note that this policy also illustrates how to cover drop entries in any case although this may not strictly conform to the Pod Security Standards.</p> <p>Rules</p> <ul> <li>require-drop-cap-net-raw (Validation)</li> </ul> <p>Total Policies: 10</p>"},{"location":"platform/security/pod-security/","title":"Pod Security Policies","text":"<p>Pod Security Admission (PSA) is a built-in admission controller in Kubernetes that enforces Pod Security Standards (PSS) at the namespace level. PSA helps ensure that pods running in your cluster adhere to security best practices by applying predefined security policies.</p>"},{"location":"platform/security/pod-security/#pod-security-standards","title":"Pod Security Standards","text":"<p>Kubernetes defines three levels of Pod Security Standards:</p> <ul> <li>Privileged: Provides the least restrictive policies, allowing all capabilities and access.</li> <li>Baseline: Provides a reasonable set of restrictions that prevent known privilege escalation and the most common security risks.</li> <li>Restricted: Provides the most restrictive policies, enforcing best practices for security.</li> </ul>"},{"location":"platform/security/pod-security/#namespace-policies","title":"Namespace Policies","text":"<p>By default, all namespaces, including system namespaces, will use the Baseline policy. However, you can change the policy for a specific namespace by setting the <code>pod-security</code> field in the namespace options. Note, as of now ONLY system application can change the PSA level i.e those deployed by <code>workloads/system</code>; standard tenant application default to <code>baseline</code>.</p> <p>Example:</p> <pre><code>## For helm applications\nhelm: ...\n## Namespace configuration\nnamespace:\n  ## Override pod security policy for the namespace, default is Baseline\n  pod_security: restricted\n</code></pre> <p>For kustomize applications, you can set the <code>pod-security</code> field in the namespace options.</p> <p>Example:</p> <pre><code>kustomize: {}\n## Namespace configuration\nnamespace:\n  ## Override pod security policy for the namespace, default is Baseline\n  pod_security: restricted\n</code></pre> <p>Note for security reason all tenant applications MUST run using the baseline security policy, which cannot be changed. Only system applications can change their security posture.</p>"},{"location":"platform/tenant/applications/","title":"Tenant Applications","text":"<p>Note</p> <p>Please refer to the architectural overview for an understanding on tenant and platform repositories</p> <p>Applications for tenants can be deployed using a GitOps approach directly from the tenant repository. The workloads folder contains two main directories:</p> <ul> <li>workloads/applications/ - Contains standard application definitions that run under the tenant's ArgoCD project with regular permissions</li> <li>workloads/system/ - Contains system-level application definitions that run under a privileged ArgoCD project with elevated permissions</li> </ul> <p>By simply adding Helm charts or Kustomize configurations into the appropriate directory structure, applications can be:</p> <ul> <li>Easily deployed to the cluster</li> <li>Upgraded through GitOps workflows</li> <li>Promoted between environments in a controlled manner</li> </ul> <p>This separation of applications and system components allows for proper access control while maintaining a simple deployment model.</p>"},{"location":"platform/tenant/applications/#helm-applications","title":"Helm Applications","text":"<p>You can deploy using a helm chart, by adding a <code>CLUSTER_NAME.yaml</code>.</p> <ol> <li>Create a folder (by default this becomes the namespace)</li> <li>Add a <code>CLUSTER_NAME.yaml</code> file</li> </ol> <pre><code>helm:\n  ## (Optional) The chart to use for the deployment.\n  chart: ./charts/platform\n  ## (Optional) The path inside a repository to the chart to use for the deployment.\n  path: ./charts/platform\n  ## (Required) The release name to use for the deployment.\n  release_name: platform\n  ## (Required) The version of the chart to use for the deployment.\n  version: 0.1.0\n  ## (Optional) A collection of additional parameters - note these can reference metadata\n  ## from the selected cluster definition.\n  parameters:\n    - name: serviceAccount.annotations.test\n      value: default_value\n    # Reference metadata from the cluster definition\n    - name: serviceAccount.annotations.test2\n      value: metadata.labels.cloud_vendor\n\n## Sync Options\nsync:\n  # (Optional) The phase to use for the deployment, used to determine the order of the deployment.\n  phase: primary|secondary\n  # (Optional) The duration to use for the deployment.\n  duration: 30s\n  # (Optional) The max duration to use for the deployment.\n  max_duration: 5m\n</code></pre> <p>In order to use helm values, you need to create a <code>values.yaml</code> file.</p> <ol> <li>For the helm values, create a folder called <code>values</code> inside the folder you created in step 1.</li> <li>Add a <code>all.yaml</code> file to the values folder, which will be used to deploy the application.</li> </ol>"},{"location":"platform/tenant/applications/#helm-with-multiple-charts","title":"Helm with Multiple Charts","text":"<p>Similar to the helm deployment, create a folder for your deployments. Taking the example of two charts, frontend and backend, you would create a folder called <code>frontend</code> and <code>backend</code>.</p> <ol> <li>Create a folder called for the application, e.g. <code>myapp</code></li> <li>Create two folders inside the <code>myapp</code> folder, <code>frontend</code> and <code>backend</code></li> <li>Add a <code>helm.yaml</code> file to the <code>frontend</code> folder.</li> <li>You can same format as above for the <code>helm.yaml</code> file.</li> <li>Add a <code>values</code> folder to the <code>frontend</code> folder, and add a <code>all.yaml</code> file to the values folder.</li> <li>Add a <code>values</code> folder to the <code>backend</code> folder, and add a <code>all.yaml</code> file to the values folder.</li> </ol>"},{"location":"platform/tenant/applications/#kustomize","title":"Kustomize","text":"<p>You can deploy using kustomize, by adding a <code>CLUSTER_NAME.yaml</code>.</p> <ol> <li>Create a folder (by default this becomes the namespace)</li> <li>Add the <code>CLUSTER_NAME.yaml</code> file</li> </ol> <pre><code>kustomize:\n  # (Required) The path to the kustomize base.\n  path: kustomize\n  # (Optional) Override the namespace to use for the deployment.\n  namespace: override-namespace\n  # (Required) Details the revision to point; this is a revision within those repository and\n  # is used to control a point in time of the manifests.\n  revision: &lt;GIT+SHA&gt;\n  # (Optional) Patches to apply to the deployment.\n  patches:\n    - target:\n        kind: Deployment\n        name: frontend\n      patch:\n        - op: replace\n          path: /spec/template/spec/containers/0/image\n          ## This value is looked from the cluster definition.\n          value: metadata.annotations.image\n          ## This is the default value to use if the value is not found.\n          default: nginx:1.21.3\n        - op: replace\n          path: /spec/template/spec/containers/0/version\n          ## This value is looked from the cluster definition.\n          value: metadata.annotations.version\n          ## This is the default value to use if the value is not found.\n          default: \"1.21.3\"\n\n  ## Optional labels applied to all resources\n  commonLabels:\n    app.kubernetes.io/managed-by: argocd\n\n  ## Optional annotations applied to all resources\n  commonAnnotations:\n    argocd.argoproj.io/sync-options: Prune=false\n</code></pre> <p>Unlike Helm where versions are managed externally through chart repositories, Kustomize manifests are typically stored directly in your repository. While Kustomize overlays provide environment-specific customization, changes to shared base configurations could potentially affect all environments simultaneously.</p> <p>To provide better control and safety, the <code>revision</code> field is used to pin Kustomize deployments to a specific Git commit or branch in the tenant repository. This allows you to:</p> <ul> <li>Make changes to manifests in the main branch without affecting production</li> <li>Control the rollout of changes across environments by updating revisions</li> <li>Roll back to previous versions by reverting to earlier commits</li> <li>Test changes in lower environments before promoting to production</li> </ul> <p>Example workflow:</p> <ol> <li>Develop and commit Kustomize changes to main branch</li> <li>Test in dev environment by updating dev cluster's revision</li> <li>Promote to staging/prod by updating their revisions after validation</li> <li>Roll back if needed by reverting to previous commit SHA</li> </ol>"},{"location":"platform/tenant/applications/#kustomize-with-external-source","title":"Kustomize with External Source","text":"<p>For teams that prefer to maintain their Kustomize manifests in a separate repository, you can reference external sources directly. This provides flexibility in managing deployment configurations and allows for independent versioning strategies. A typical example would be to use floating tags to represent the environments.</p> <ol> <li>Create a folder for your application (this becomes the namespace by default)</li> <li>Add the <code>CLUSTER_NAME.yaml</code> file with external repository configuration:</li> </ol> <pre><code>kustomize:\n  # (Required) The URL to the kustomize repository\n  repository: GIT_URL\n  # (Required) The path inside the repository\n  path: overlays/dev\n  # (Required) Use a floating tag 'dev' for the development cluster and similar for the prod\n  revision: dev\n</code></pre>"},{"location":"platform/tenant/applications/#combinational-deployment","title":"Combinational Deployment","text":"<p>You can combine both helm and kustomize deployments in a single file. This allows you to deploy applications that require both deployment methods.</p> <ol> <li>Create a folder for your application, e.g. <code>myapp</code></li> <li>Add a <code>CLUSTER_NAME.yaml</code> file that contains both helm and kustomize configurations</li> </ol> <pre><code>helm:\n  ## (Optional) The chart to use for the deployment.\n  chart: ./charts/platform\n  ## (Optional) The path inside a repository to the chart to use for the deployment.\n  path: ./charts/platform\n  ## (Required) The release name to use for the deployment.\n  release_name: platform\n  ## (Required) The version of the chart to use for the deployment.\n  version: 0.1.0\n\nkustomize:\n  # (Required) The path to the kustomize base.\n  path: kustomize\n  # (Optional) Override the namespace to use for the deployment.\n  namespace: override-namespace\n  # (Required) Git revision\n  revision: git+sha\n</code></pre>"},{"location":"platform/tenant/system/","title":"Tenant System Applications","text":"<p>Note</p> <p>Please refer to the architectural overview for an understanding on tenant and platform repositories</p> <p>System applications deployed under <code>workloads/system/</code> have elevated privileges compared to regular applications. These system-level applications:</p> <ul> <li>Can deploy cluster-scoped resources (ClusterRoles, CustomResourceDefinitions, etc.)</li> <li>Run under a privileged ArgoCD project with higher permissions</li> <li>Are typically used for infrastructure and platform components</li> <li>Have access beyond namespace boundaries</li> </ul> <p>This higher privilege level allows system applications to:</p> <ul> <li>Install cluster-wide operators and controllers</li> <li>Configure cluster-level security policies</li> <li>Set up monitoring and logging infrastructure</li> <li>Deploy shared services used by multiple applications</li> </ul>"},{"location":"platform/tenant/system/#usage-guidelines","title":"Usage Guidelines","text":"<p>Note</p> <p>System applications have elevated permissions and can affect the entire cluster. Use caution when deploying system applications to avoid unintended consequences.</p> <p>When deploying system applications:</p> <ol> <li>Only place applications that truly need cluster-wide access under <code>workloads/system/</code></li> <li>Regular applications should remain under <code>workloads/applications/</code> with standard namespace-scoped permissions</li> <li>Follow the same deployment formats (Helm/Kustomize) as regular applications</li> <li>Be cautious with elevated privileges to avoid unintended cluster-wide changes</li> </ol> <p>The separation between system and regular applications helps maintain proper security boundaries while enabling necessary cluster-wide functionality.</p>"},{"location":"platform/tenant/system/#namespace-override","title":"Namespace Override","text":"<p>By default, applications are deployed into a namespace matching their folder name. However, system applications can override this default namespace using the <code>namespace</code> field:</p>"},{"location":"platform/tenant/system/#helm-example","title":"Helm Example","text":"<pre><code>helm:\n  ## (Optional) The chart to use for the deployment.\n  chart: ./charts/platform\n  ## (Optional) The path inside a repository to the chart to use for the deployment.\n  path: ./charts/platform\n  ## (Required) The release name to use for the deployment.\n  release_name: platform\n  ## (Required) The version of the chart to use for the deployment.\n  version: 0.1.0\n\nnamespace:\n  ## Override the namespace\n  name: kube-system\n</code></pre>"}]}